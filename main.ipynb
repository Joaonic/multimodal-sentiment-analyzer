{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b89035",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Análise de Sentimentos Multimodal com Diarização e Aprendizado Contínuo\n",
    "\n",
    "Este notebook implementa um pipeline completo que:\n",
    "- Lê um vídeo ou stream (áudio e vídeo) e extrai frames e áudio.\n",
    "- Realiza **diarização** do áudio utilizando o *pyannote.audio*.\n",
    "- Extrai a emoção facial de cada frame com o *DeepFace*.\n",
    "- Realiza a análise de emoção do áudio com um modelo pré‑treinado do *SpeechBrain*.\n",
    "- Transcreve o áudio com *Whisper* e classifica a emoção do texto com um modelo do *Transformers*.\n",
    "- Converte cada uma dessas análises em vetores de probabilidade (7 dimensões: _angry, disgust, fear, happy, sad, surprise, neutral_).\n",
    "- Usa um **modelo de fusão** (rede neural em PyTorch) que recebe os 3 vetores unimodais (concatenados em um vetor de dimensão 21) e gera uma previsão final. Esse modelo é treinado continuamente de forma auto‑supervisionada, utilizando como pseudo‑label a média dos três vetores.\n",
    "- Agrega os resultados por locutor e gera uma saída JSON, incluindo \"padrões\" extraídos (ex.: detecção de consistência de emoção).\n",
    "\n",
    "O pipeline roda em dois modos:\n",
    "1. **Offline:** Processa um vídeo gravado.\n",
    "2. **Streaming:** Captura áudio e vídeo em tempo real (utilizando webcam e microfone via PyAudio) e processa em blocos de tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45571dd",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Instalação das Dependências\n",
    "\n",
    "Execute os seguintes comandos para instalar as dependências necessárias:\n",
    "```bash\n",
    "pip install opencv-python torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install pyaudio\n",
    "pip install openai-whisper\n",
    "pip install deepface\n",
    "pip install pyannote.audio\n",
    "pip install speechbrain\n",
    "pip install transformers\n",
    "pip install ffmpeg-python\n",
    "\n",
    "# Instalação do FFmpeg (necessário para processamento de áudio/vídeo)\n",
    "apt-get install ffmpeg\n",
    "```\n",
    "\n",
    "Observações importantes:\n",
    "1. Você precisará reiniciar o runtime após instalar as dependências\n",
    "2. A instalação do PyAudio pode apresentar problemas no Colab. Se isso acontecer, tente:\n",
    "   ```bash\n",
    "   apt-get install portaudio19-dev\n",
    "   pip install pyaudio\n",
    "   ```\n",
    "3. Para usar o modo streaming, você precisará permitir o acesso à webcam\n",
    "4. Configure seu token do HuggingFace na variável HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459cb315",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# 0. CONFIGURAÇÕES DE MEMÓRIA CUDA"
   ]
  },
  {
   "cell_type": "code",
   "id": "4e673420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T21:37:25.478090Z",
     "start_time": "2025-04-22T21:37:25.451526Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "# Enable expandable segments to reduce fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import queue\n",
    "import subprocess\n",
    "import time\n",
    "import wave\n",
    "\n",
    "# Importação das bibliotecas necessárias\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import whisper\n",
    "from deepface import DeepFace\n",
    "from pyannote.audio import Pipeline\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from transformers import pipeline as hf_pipeline\n",
    "from tqdm import tqdm  # Para barras de progresso\n",
    "\n",
    "# Configurações CUDA\n",
    "print(f\"CUDA disponível: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Dispositivo CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"PyTorch versão: {torch.__version__}\")\n",
    "print(f\"OpenCV versão: {cv2.__version__}\")\n",
    "try:\n",
    "    print(f\"Whisper versão: {whisper.__version__}\")\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Otimização cuDNN\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# Configurações CUDA e tipos de dados\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduz logs do TensorFlow\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Desabilita oneDNN\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Força uso da GPU 0\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'  # Habilita XLA\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'  # Otimiza threads GPU"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mwave\u001B[39;00m\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# Importação das bibliotecas necessárias\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcv2\u001B[39;00m\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyaudio\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'cv2'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "b2b66157",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 1. Parâmetros e Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9820b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\"  # Substitua pelo seu token do HuggingFace\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Utilizando {DEVICE}\")\n",
    "\n",
    "# Carregamento dos modelos com tipos de dados corretos\n",
    "asr_model = whisper.load_model(\"medium\", device=DEVICE).to(DEVICE).float()  # Usando float32 ao invés de half\n",
    "\n",
    "SER_MODEL = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\",\n",
    "    savedir=\"pretrained_models/ser\",\n",
    "    run_opts={\"device\": DEVICE}\n",
    ").to(DEVICE).eval().float()  # Usando float32 ao invés de half\n",
    "\n",
    "text_emotion_pipeline = hf_pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    return_all_scores=True,\n",
    "    device=0 if DEVICE == \"cuda\" else -1,\n",
    "    torch_dtype=torch.float32  # Usando float32 ao invés de half\n",
    ")\n",
    "\n",
    "# Configurações de memória\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484df584",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 2. Funções de Pré-processamento\n",
    "\n",
    "Funções para extrair o áudio de um vídeo (usando FFmpeg) e para carregar os frames e timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4642d2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_audio(video_path, audio_output=None):\n",
    "    \"\"\"\n",
    "    Extrai o áudio do vídeo utilizando FFmpeg e salva em WAV (16 kHz mono).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verifica se o arquivo de vídeo existe\n",
    "        if not os.path.exists(video_path):\n",
    "            raise FileNotFoundError(f\"Arquivo de vídeo não encontrado: {video_path}\")\n",
    "            \n",
    "        # Define o caminho do arquivo de áudio\n",
    "        if audio_output is None:\n",
    "            # Usa o mesmo diretório do vídeo com nome padrão\n",
    "            video_dir = os.path.dirname(video_path)\n",
    "            audio_output = os.path.join(video_dir, \"extracted_audio.wav\")\n",
    "        else:\n",
    "            # Converte para caminho absoluto\n",
    "            audio_output = os.path.abspath(audio_output)\n",
    "            \n",
    "        # Cria o diretório se não existir\n",
    "        os.makedirs(os.path.dirname(audio_output), exist_ok=True)\n",
    "        \n",
    "        print(f\"Extraindo áudio para: {audio_output}\")\n",
    "        \n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-y\", \"-i\", video_path, \"-vn\",\n",
    "            \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\",\n",
    "            audio_output\n",
    "        ]\n",
    "        \n",
    "        # Executa o comando e captura a saída\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        \n",
    "        # Verifica se o comando foi executado com sucesso\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(f\"Erro ao extrair áudio: {result.stderr}\")\n",
    "            \n",
    "        # Verifica se o arquivo foi criado\n",
    "        if not os.path.exists(audio_output):\n",
    "            raise RuntimeError(f\"Arquivo de áudio não foi criado: {audio_output}\")\n",
    "            \n",
    "        print(f\"Áudio extraído com sucesso: {audio_output}\")\n",
    "        return audio_output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair áudio: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_video_frames(video_path):\n",
    "    \"\"\"\n",
    "    Carrega os frames do vídeo com OpenCV e retorna:\n",
    "    - frames: lista de imagens (numpy arrays)\n",
    "    - timestamps: lista de tempos (em segundos) para cada frame\n",
    "    - fps: frames por segundo do vídeo\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    timestamps = []\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_index = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "        timestamps.append(frame_index / fps)\n",
    "        frame_index += 1\n",
    "    cap.release()\n",
    "    return frames, timestamps, fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d1995",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 3. Diarização de Áudio\n",
    "\n",
    "Utiliza o pipeline do *pyannote.audio* para segmentar o áudio por locutor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b6924",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_diarization(audio_path, hf_token):\n",
    "    \"\"\"\n",
    "    Executa a diarização no áudio e retorna uma lista de segmentos:\n",
    "    Cada segmento é um dicionário com: start, end, speaker.\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=hf_token)\n",
    "    pipeline.to(torch.device(DEVICE))\n",
    "    diarization = pipeline(audio_path)\n",
    "\n",
    "    segments = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        segments.append({\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end,\n",
    "            \"speaker\": speaker\n",
    "        })\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c542f691",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 4. Análise Unimodal\n",
    "\n",
    "### 4.1 Análise Facial\n",
    "Extrai um vetor de probabilidade (7 dimensões) para as emoções usando DeepFace.\n",
    "### 4.2 Análise de Áudio\n",
    "Utiliza um modelo pré‑treinado do SpeechBrain para reconhecimento de emoção no áudio.\n",
    "### 4.3 Análise de Texto\n",
    "Transcreve o áudio com Whisper e classifica a emoção do texto utilizando um pipeline do Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485daf25",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_face_emotion_vector(frame):\n",
    "    \"\"\"\n",
    "    Analisa a emoção no frame utilizando DeepFace e retorna um vetor de 7 dimensões.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configuração do DeepFace para usar GPU\n",
    "        result = DeepFace.analyze(\n",
    "            frame,\n",
    "            actions=['emotion'],\n",
    "            enforce_detection=False,\n",
    "            detector_backend='opencv',\n",
    "            silent=True,  # Desabilita logs\n",
    "            prog_bar=False  # Desabilita barra de progresso\n",
    "        )\n",
    "        \n",
    "        if isinstance(result, list):\n",
    "            result = result[0]\n",
    "            \n",
    "        emotion_dict = result.get(\"emotion\", {})\n",
    "        face_vec = np.array([\n",
    "            emotion_dict.get(\"angry\", 0),\n",
    "            emotion_dict.get(\"disgust\", 0),\n",
    "            emotion_dict.get(\"fear\", 0),\n",
    "            emotion_dict.get(\"happy\", 0),\n",
    "            emotion_dict.get(\"sad\", 0),\n",
    "            emotion_dict.get(\"surprise\", 0),\n",
    "            emotion_dict.get(\"neutral\", 0)\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        if np.sum(face_vec) > 0:\n",
    "            face_vec = face_vec / np.sum(face_vec)\n",
    "    except Exception as e:\n",
    "        print(\"Erro na análise facial:\", e)\n",
    "        face_vec = np.zeros(7, dtype=np.float32)\n",
    "    return face_vec\n",
    "\n",
    "\n",
    "def analyze_audio_emotion(audio_segment_path):\n",
    "    \"\"\"\n",
    "    Analisa o segmento de áudio e retorna um vetor 7-dim de probabilidades.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Obtém as probabilidades do modelo SpeechBrain\n",
    "            probs = SER_MODEL.classify_file(audio_segment_path)\n",
    "            probs = probs[0].squeeze().cpu().numpy()\n",
    "            \n",
    "        # Mapeamento para vetor 7-dim\n",
    "        audio_vec = np.zeros(7, dtype=np.float32)\n",
    "        mapping = {\n",
    "            \"angry\": 0,\n",
    "            \"happy\": 3,\n",
    "            \"neutral\": 6,\n",
    "            \"sad\": 4\n",
    "        }\n",
    "        \n",
    "        for i, label in enumerate([\"angry\", \"happy\", \"neutral\", \"sad\"]):\n",
    "            audio_vec[mapping[label]] = probs[i]\n",
    "            \n",
    "        if np.sum(audio_vec) > 0:\n",
    "            audio_vec = audio_vec / np.sum(audio_vec)\n",
    "    except Exception as e:\n",
    "        print(\"Erro na análise de áudio (SER):\", e)\n",
    "        audio_vec = np.zeros(7, dtype=np.float32)\n",
    "    return audio_vec\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_segment_path):\n",
    "    \"\"\"\n",
    "    Transcreve o áudio utilizando o modelo Whisper (em português).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = asr_model.transcribe(\n",
    "            audio_segment_path,\n",
    "            language=\"pt\",\n",
    "            fp16=False  # Desabilitando fp16 para evitar problemas de tipo\n",
    "        )\n",
    "        return result[\"text\"]\n",
    "    except Exception as e:\n",
    "        print(\"Erro na transcrição:\", e)\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def analyze_text_emotion(text):\n",
    "    \"\"\"\n",
    "    Analisa a emoção do texto e retorna um vetor 7-dim com probabilidades.\n",
    "    Mapeamento de labels do pipeline para as posições:\n",
    "      anger->0, disgust->1, fear->2, joy->3, sad->4, surprise->5, neutral->6\n",
    "    \"\"\"\n",
    "    results = text_emotion_pipeline(text)[0]\n",
    "    text_vec = np.zeros(7, dtype=np.float32)\n",
    "    mapping = {\n",
    "        \"anger\": \"angry\",\n",
    "        \"disgust\": \"disgust\",\n",
    "        \"fear\": \"fear\",\n",
    "        \"joy\": \"happy\",\n",
    "        \"sadness\": \"sad\",\n",
    "        \"surprise\": \"surprise\",\n",
    "        \"neutral\": \"neutral\"\n",
    "    }\n",
    "    labels = [\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
    "    for item in results:\n",
    "        mapped = mapping.get(item[\"label\"].lower())\n",
    "        if mapped:\n",
    "            idx = labels.index(mapped)\n",
    "            text_vec[idx] = item[\"score\"]\n",
    "    if text_vec.sum() > 0:\n",
    "        text_vec /= text_vec.sum()\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1242e74c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 5. Modelo de Fusão de IA\n",
    "\n",
    "Rede neural em PyTorch que recebe um vetor concat (21-dim) e gera uma previsão final (7-dim).\n",
    "O treinamento é contínuo, com pseudo‑label igual à média dos 3 vetores unimodais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2b767",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, input_dim=21, hidden_dim=128, output_dim=7):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return F.softmax(self.fc3(x), dim=1)\n",
    "\n",
    "\n",
    "fusion_model = FusionModel().to(DEVICE).float()\n",
    "optimizer = optim.AdamW(fusion_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "\n",
    "def update_fusion_model(face_vec, audio_vec, text_vec):\n",
    "    \"\"\"\n",
    "    Atualiza o modelo de fusão com os vetores de entrada e retorna o vetor de saída (7-dim).\n",
    "    \"\"\"\n",
    "    inp = np.concatenate([face_vec, audio_vec, text_vec]).astype(np.float32)\n",
    "    inp_t = torch.tensor(inp, device=DEVICE).unsqueeze(0).float()\n",
    "    target = (face_vec + audio_vec + text_vec) / 3\n",
    "    target = target / (target.sum() if target.sum() > 0 else 1)\n",
    "    tgt_t = torch.tensor(target, device=DEVICE).unsqueeze(0).float()\n",
    "\n",
    "    fusion_model.train()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        out = fusion_model(inp_t)\n",
    "\n",
    "    loss = loss_fn(torch.log(out + 1e-8), tgt_t)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del inp_t, tgt_t, loss\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return out.detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae285e7e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 6. Processamento de Segmentos com Fusão\n",
    "\n",
    "Para cada segmento:\n",
    "- Seleciona frame no meio do segmento.\n",
    "- Extrai snippet de áudio.\n",
    "- Executa análises facial, áudio, texto.\n",
    "- Atualiza modelo de fusão e padrões do locutor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa283393",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_frame_at_time(video_path, time_sec):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, time_sec * 1000)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    return frame, fps\n",
    "\n",
    "\n",
    "def process_segment_fusion(segment, video_path, audio_file, speaker_patterns):\n",
    "    start, end, speaker = segment[\"start\"], segment[\"end\"], segment[\"speaker\"]\n",
    "    mid_time = (start + end) / 2\n",
    "    frame, _ = get_frame_at_time(video_path, mid_time)\n",
    "\n",
    "    face_vec = analyze_face_emotion_vector(frame)\n",
    "\n",
    "    snippet_path = f\"snippet_{start:.2f}_{end:.2f}.wav\"\n",
    "    subprocess.run([\n",
    "        \"ffmpeg\", \"-y\", \"-i\", audio_file,\n",
    "        \"-ss\", str(start), \"-to\", str(end),\n",
    "        \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\",\n",
    "        snippet_path\n",
    "    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    audio_vec = analyze_audio_emotion(snippet_path)\n",
    "    transcript = transcribe_audio(snippet_path)\n",
    "    text_vec = analyze_text_emotion(transcript)\n",
    "\n",
    "    if os.path.exists(snippet_path):\n",
    "        os.remove(snippet_path)\n",
    "\n",
    "    fused_vec = update_fusion_model(face_vec, audio_vec, text_vec)\n",
    "    emo_idx = int(np.argmax(fused_vec))\n",
    "    emotion_labels = [\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
    "    fused_emotion = emotion_labels[emo_idx]\n",
    "\n",
    "    speaker_patterns.setdefault(speaker, []).append((start, end, fused_emotion))\n",
    "\n",
    "    return {\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "        \"speaker\": speaker,\n",
    "        \"face_vec\": face_vec.tolist(),\n",
    "        \"audio_vec\": audio_vec.tolist(),\n",
    "        \"text_vec\": text_vec.tolist(),\n",
    "        \"transcript\": transcript,\n",
    "        \"fused_vec\": fused_vec.tolist(),\n",
    "        \"fused_emotion\": fused_emotion\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783eddfb",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 7. Pipeline Offline Completo\n",
    "\n",
    "- Extrai áudio.\n",
    "- Diariza.\n",
    "- Processa segmentos.\n",
    "- Agrega resultados por locutor e patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab67930",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_video_offline_fusion(video_path, hf_token):\n",
    "    try:\n",
    "        print(\"Extraindo áudio do vídeo...\")\n",
    "        # Usa o mesmo diretório do vídeo para o áudio\n",
    "        video_dir = os.path.dirname(video_path)\n",
    "        audio_file = extract_audio(video_path, os.path.join(video_dir, \"extracted_audio.wav\"))\n",
    "        \n",
    "        if not os.path.exists(audio_file):\n",
    "            raise FileNotFoundError(f\"Arquivo de áudio não encontrado após extração: {audio_file}\")\n",
    "\n",
    "        print(\"Executando diarização...\")\n",
    "        segments = perform_diarization(audio_file, hf_token)\n",
    "\n",
    "        results = []\n",
    "        speaker_patterns = {}\n",
    "        print(\"Processando segmentos com fusão...\")\n",
    "        \n",
    "        # Adiciona barra de progresso para os segmentos\n",
    "        for seg in tqdm(segments, desc=\"Processando segmentos\"):\n",
    "            print(f\"\\nProcessando segmento: {seg}\")\n",
    "            res = process_segment_fusion(seg, video_path, audio_file, speaker_patterns)\n",
    "            results.append(res)\n",
    "\n",
    "        if os.path.exists(audio_file):\n",
    "            os.remove(audio_file)\n",
    "\n",
    "        speakers_data = {}\n",
    "        for res in results:\n",
    "            spk = res[\"speaker\"]\n",
    "            speakers_data.setdefault(spk, {\"segmentos\": [], \"emocao_segmentos\": [], \"padroes\": []})\n",
    "            speakers_data[spk][\"segmentos\"].append({\"inicio\": res[\"start\"], \"fim\": res[\"end\"]})\n",
    "            speakers_data[spk][\"emocao_segmentos\"].append({\n",
    "                \"tempo\": [res[\"start\"], res[\"end\"]],\n",
    "                \"emocao\": res[\"fused_emotion\"],\n",
    "                \"vetor\": res[\"fused_vec\"]\n",
    "            })\n",
    "\n",
    "        for spk, segs in speaker_patterns.items():\n",
    "            emotions = [emo for (_, _, emo) in segs]\n",
    "            for i in range(len(emotions) - 2):\n",
    "                if emotions[i] == emotions[i + 1] == emotions[i + 2]:\n",
    "                    pattern = f\"Emoção consistente '{emotions[i]}' nos segmentos {i+1}-{i+3}\"\n",
    "                    speakers_data[spk][\"padroes\"].append(pattern)\n",
    "\n",
    "        output = []\n",
    "        for spk, data in speakers_data.items():\n",
    "            counts = {}\n",
    "            for seg in data[\"emocao_segmentos\"]:\n",
    "                counts[seg[\"emocao\"]] = counts.get(seg[\"emocao\"], 0) + 1\n",
    "            dominant_emotion = max(counts, key=counts.get) if counts else \"unknown\"\n",
    "            output.append({\n",
    "                \"pessoa\": spk,\n",
    "                \"segmentos\": data[\"segmentos\"],\n",
    "                \"emocao_dominante\": dominant_emotion,\n",
    "                \"emocao_segmentos\": data[\"emocao_segmentos\"],\n",
    "                \"padroes\": data[\"padroes\"]\n",
    "            })\n",
    "\n",
    "        return output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no processamento do vídeo: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1554b0a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 8. Pipeline de Streaming Completo\n",
    "\n",
    "Captura em tempo real via PyAudio e OpenCV, processa em blocos de `duration` segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698273b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_streaming_fusion(duration=5):\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 16000\n",
    "    CHUNK = 1024\n",
    "\n",
    "    audio_buffer = queue.Queue()\n",
    "\n",
    "    def audio_callback(in_data, frame_count, time_info, status):\n",
    "        audio_buffer.put(in_data)\n",
    "        return (in_data, pyaudio.paContinue)\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,\n",
    "                    input=True, frames_per_buffer=CHUNK,\n",
    "                    stream_callback=audio_callback)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    captured_frames = []\n",
    "    start_time = time.time()\n",
    "    segment_results = []\n",
    "    speaker_patterns = {}\n",
    "\n",
    "    print(\"Iniciando streaming multimodal. Pressione 'q' na janela de vídeo para encerrar.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            captured_frames.append(frame)\n",
    "            cv2.imshow(\"Streaming\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        if time.time() - start_time >= duration:\n",
    "            frames_audio = []\n",
    "            while not audio_buffer.empty():\n",
    "                frames_audio.append(audio_buffer.get())\n",
    "            audio_filename = \"stream_segment.wav\"\n",
    "            wf = wave.open(audio_filename, 'wb')\n",
    "            wf.setnchannels(CHANNELS)\n",
    "            wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "            wf.setframerate(RATE)\n",
    "            wf.writeframes(b''.join(frames_audio))\n",
    "            wf.close()\n",
    "\n",
    "            temp_video_file = \"stream_video_temp.mp4\"\n",
    "            if captured_frames:\n",
    "                h, w, _ = captured_frames[0].shape\n",
    "                fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "                out = cv2.VideoWriter(temp_video_file, fourcc, 20.0, (w, h))\n",
    "                for frm in captured_frames:\n",
    "                    out.write(frm)\n",
    "                out.release()\n",
    "            else:\n",
    "                dummy = np.zeros((480, 640, 3), np.uint8)\n",
    "                h, w, _ = dummy.shape\n",
    "                fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "                out = cv2.VideoWriter(temp_video_file, fourcc, 20.0, (w, h))\n",
    "                out.write(dummy)\n",
    "                out.release()\n",
    "\n",
    "            segments = perform_diarization(audio_filename, HF_TOKEN)\n",
    "            if not segments:\n",
    "                segments = [{\"start\": 0, \"end\": duration, \"speaker\": \"stream_speaker\"}]\n",
    "\n",
    "            for seg in segments:\n",
    "                res = process_segment_fusion(seg, temp_video_file, audio_filename, speaker_patterns)\n",
    "                segment_results.append(res)\n",
    "                print(\"Segmento processado:\", res)\n",
    "\n",
    "            start_time = time.time()\n",
    "            captured_frames = []\n",
    "            if os.path.exists(audio_filename):\n",
    "                os.remove(audio_filename)\n",
    "            if os.path.exists(temp_video_file):\n",
    "                os.remove(temp_video_file)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    speakers_data = {}\n",
    "    for res in segment_results:\n",
    "        spk = res[\"speaker\"]\n",
    "        speakers_data.setdefault(spk, {\"segmentos\": [], \"emocao_segmentos\": [], \"padroes\": []})\n",
    "        speakers_data[spk][\"segmentos\"].append({\"inicio\": res[\"start\"], \"fim\": res[\"end\"]})\n",
    "        speakers_data[spk][\"emocao_segmentos\"].append({\n",
    "            \"tempo\": [res[\"start\"], res[\"end\"]],\n",
    "            \"emocao\": res[\"fused_emotion\"],\n",
    "            \"vetor\": res[\"fused_vec\"]\n",
    "        })\n",
    "\n",
    "    for spk, segs in speaker_patterns.items():\n",
    "        emotions = [emo for (_, _, emo) in segs]\n",
    "        for i in range(len(emotions) - 2):\n",
    "            if emotions[i] == emotions[i + 1] == emotions[i + 2]:\n",
    "                pattern = f\"Emoção consistente '{emotions[i]}' nos segmentos {i+1}-{i+3}\"\n",
    "                speakers_data[spk][\"padroes\"].append(pattern)\n",
    "\n",
    "    output = []\n",
    "    for spk, data in speakers_data.items():\n",
    "        counts = {}\n",
    "        for seg in data[\"emocao_segmentos\"]:\n",
    "            counts[seg[\"emocao\"]] = counts.get(seg[\"emocao\"], 0) + 1\n",
    "        dominant_emotion = max(counts, key=counts.get) if counts else \"unknown\"\n",
    "        output.append({\n",
    "            \"pessoa\": spk,\n",
    "            \"segmentos\": data[\"segmentos\"],\n",
    "            \"emocao_dominante\": dominant_emotion,\n",
    "            \"emocao_segmentos\": data[\"emocao_segmentos\"],\n",
    "            \"padroes\": data[\"padroes\"]\n",
    "        })\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94944d68",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 9. Execução Principal\n",
    "\n",
    "Escolha o modo de execução:\n",
    "- `\"offline\"`: Processa um vídeo gravado (defina o caminho do vídeo).\n",
    "- `\"streaming\"`: Executa a captura ao vivo (pressione 'q' na janela de vídeo para encerrar).\n",
    "\n",
    "A saída final é impressa em JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2087fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    mode = \"offline\"  # Altere para \"streaming\" para testar o modo ao vivo.\n",
    "\n",
    "    if mode == \"offline\":\n",
    "        video_file = \"/home/joao/yo/multimodal-sentiment-analyzer/teste.mp4\"  # Defina seu caminho\n",
    "        resultado = process_video_offline_fusion(video_file, HF_TOKEN)\n",
    "        print(\"Resultado JSON:\")\n",
    "        print(json.dumps(resultado, indent=2, ensure_ascii=False))\n",
    "    else:\n",
    "        resultado = process_streaming_fusion(duration=5)\n",
    "        print(\"Resultado JSON:\")\n",
    "        print(json.dumps(resultado, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb490188",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Documentação das Funções Principais\n",
    "\n",
    "## Funções de Processamento de Vídeo\n",
    "\n",
    "### `extract_audio(video_path: str, audio_output: str) -> str`\n",
    "Extrai o áudio do vídeo para análise.\n",
    "\n",
    "### `load_video_frames(video_path: str) -> (List[np.ndarray], List[float], float)`\n",
    "Extrai frames e timestamps.\n",
    "\n",
    "## Funções de Processamento de Áudio\n",
    "\n",
    "### `analyze_audio_emotion(audio_segment_path: str) -> np.ndarray`\n",
    "Retorna vetor de emoções (7-dim).\n",
    "\n",
    "## Funções de Transcrição e Diarização\n",
    "\n",
    "### `transcribe_audio(audio_segment_path: str) -> str`\n",
    "Transcreve o áudio via Whisper.\n",
    "\n",
    "### `perform_diarization(audio_path: str, hf_token: str) -> List[Dict]`\n",
    "Segmenta áudio por locutor.\n",
    "\n",
    "## Funções de Análise de Sentimento\n",
    "\n",
    "### `analyze_face_emotion_vector(frame: np.ndarray) -> np.ndarray`\n",
    "Analisa emoções faciais.\n",
    "\n",
    "### `analyze_text_emotion(text: str) -> np.ndarray`\n",
    "Analisa emoções do texto.\n",
    "\n",
    "### `update_fusion_model(face_vec: np.ndarray, audio_vec: np.ndarray, text_vec: np.ndarray) -> np.ndarray`\n",
    "Treina o modelo de fusão em loop contínuo.\n",
    "\n",
    "## Pipelines\n",
    "\n",
    "- `process_video_offline_fusion`\n",
    "- `process_streaming_fusion`"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
