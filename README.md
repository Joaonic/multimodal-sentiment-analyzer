# AnÃ¡lise de Sentimentos Multimodal

Sistema de anÃ¡lise de sentimentos que combina informaÃ§Ãµes de expressÃµes faciais, Ã¡udio e texto para identificar emoÃ§Ãµes em vÃ­deos e streaming em tempo real.

## ğŸš€ VisÃ£o Geral

O sistema utiliza trÃªs modalidades principais para anÃ¡lise de sentimentos:
- **AnÃ¡lise Facial**: Detecta expressÃµes faciais, micro-expressÃµes, direÃ§Ã£o do olhar e tensÃ£o muscular
- **AnÃ¡lise de Ãudio**: Analisa emoÃ§Ãµes, pitch, intensidade, timbre, velocidade da fala e ritmo
- **AnÃ¡lise de Texto**: Processa emoÃ§Ãµes, sarcasmo, humor, polaridade e embedding contextual

Os resultados sÃ£o combinados atravÃ©s de um modelo de fusÃ£o neural avanÃ§ado que aprende continuamente com os dados.

## ğŸ“ Estrutura do Projeto

```
multimodal-sentiment-analyzer/
â”œâ”€â”€ data/                  # Dados brutos e processados
â”œâ”€â”€ checkpoints/           # Modelos treinados
â”œâ”€â”€ output/               # Resultados das anÃ¡lises
â”œâ”€â”€ temp/                 # Arquivos temporÃ¡rios
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”‚   â”œâ”€â”€ face_analyzer.py    # AnÃ¡lise de emoÃ§Ãµes faciais
â”‚   â”‚   â”œâ”€â”€ text_analyzer.py    # AnÃ¡lise de emoÃ§Ãµes textuais
â”‚   â”‚   â””â”€â”€ audio_analyzer.py   # AnÃ¡lise de emoÃ§Ãµes no Ã¡udio
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ fusion_model.py     # Modelo de fusÃ£o multimodal
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”œâ”€â”€ offline_processor.py    # Processamento de vÃ­deos
â”‚   â”‚   â””â”€â”€ streaming_processor.py  # Processamento em tempo real
â”‚   â”œâ”€â”€ structures/
â”‚   â”‚   â”œâ”€â”€ analysis.py         # Estruturas de anÃ¡lise
â”‚   â”‚   â”œâ”€â”€ emotions.py         # Classes de emoÃ§Ãµes
â”‚   â”‚   â”œâ”€â”€ models.py           # Estruturas de modelos
â”‚   â”‚   â””â”€â”€ config.py           # Estruturas de configuraÃ§Ã£o
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config.py           # ConfiguraÃ§Ãµes do sistema
â”‚   â”œâ”€â”€ training/               # Scripts de treinamento
â”‚   â”œâ”€â”€ inference.py           # Script de inferÃªncia
â”‚   â””â”€â”€ main.py                # Script principal
â”œâ”€â”€ .env                      # VariÃ¡veis de ambiente
â”œâ”€â”€ requirements.txt          # DependÃªncias Python
â””â”€â”€ README.md                 # DocumentaÃ§Ã£o
```

## ğŸ› ï¸ Requisitos

- Python 3.8+
- CUDA 11.8+ (para GPU)
- FFmpeg
- PortAudio

### DependÃªncias Python

```bash
pip install -r requirements.txt
```

## âš™ï¸ ConfiguraÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/seu-usuario/multimodal-sentiment-analyzer.git
cd multimodal-sentiment-analyzer
```

2. Configure o token do HuggingFace no arquivo `.env`:
```bash
HF_TOKEN=seu_token_aqui
```

3. Ajuste as configuraÃ§Ãµes em `src/config/config.py`:
- DiretÃ³rios de entrada/saÃ­da
- Modelos a serem utilizados
- ParÃ¢metros de processamento
- ConfiguraÃ§Ãµes de streaming

## ğŸ¯ Uso

### Processamento de VÃ­deo

```python
from src.processors.offline_processor import OfflineProcessor
from src.config.config import MODEL_CONFIG, PROCESSING_CONFIG
import os

processor = OfflineProcessor(
    model_config=MODEL_CONFIG,
    processing_config=PROCESSING_CONFIG,
    hf_token=os.getenv('HF_TOKEN')
)

results = processor.process_video("video.mp4")
```

### Streaming em Tempo Real

```python
from src.processors.streaming_processor import StreamingProcessor
from src.config.config import MODEL_CONFIG, STREAMING_CONFIG

processor = StreamingProcessor(
    model_config=MODEL_CONFIG,
    streaming_config=STREAMING_CONFIG
)

def callback(result):
    print(f"EmoÃ§Ã£o detectada: {result['fused_emotion']}")

processor.run(duration=5.0, callback=callback)
```

## ğŸ“Š SaÃ­da

O sistema gera resultados estruturados incluindo:
- AnÃ¡lise facial: emoÃ§Ãµes, micro-expressÃµes, direÃ§Ã£o do olhar, tensÃ£o muscular
- AnÃ¡lise de Ã¡udio: emoÃ§Ãµes, pitch, intensidade, timbre, velocidade, ritmo
- AnÃ¡lise de texto: emoÃ§Ãµes, sarcasmo, humor, polaridade, embedding
- EmoÃ§Ã£o final apÃ³s fusÃ£o com confianÃ§a
- Timestamps e segmentos
- PadrÃµes de emoÃ§Ã£o por falante
- TranscriÃ§Ãµes do Ã¡udio

Exemplo de saÃ­da:
```json
{
  "speaker_id": "SPEAKER_01",
  "segments": [
    {
      "start_time": 0.0,
      "end_time": 5.0,
      "face_analysis": {
        "emotion_probs": [0.1, 0.7, 0.1, 0.1, 0.0, 0.0, 0.0],
        "micro_expressions": [...],
        "gaze_direction": [...],
        "muscle_tension": [...],
        "movement_patterns": [...]
      },
      "audio_analysis": {
        "emotion_probs": [0.1, 0.7, 0.1, 0.1, 0.0, 0.0, 0.0],
        "pitch": 0.5,
        "intensity": 0.8,
        "timbre": [...],
        "speech_rate": 0.6,
        "rhythm": [...]
      },
      "text_analysis": {
        "emotion_probs": [0.1, 0.7, 0.1, 0.1, 0.0, 0.0, 0.0],
        "sarcasm_score": 0.1,
        "humor_score": 0.3,
        "polarity": 0.8,
        "intensity": 0.7,
        "context_embedding": [...]
      },
      "fused_analysis": {
        "emotion_probs": [0.1, 0.7, 0.1, 0.1, 0.0, 0.0, 0.0],
        "confidence": 0.85,
        "face_weight": 0.4,
        "audio_weight": 0.3,
        "text_weight": 0.3
      },
      "transcript": "OlÃ¡, como vocÃª estÃ¡?",
      "confidence": 0.85,
      "dominant_emotion": "happy"
    }
  ],
  "dominant_emotion": "happy",
  "emotion_patterns": [
    "EmoÃ§Ã£o consistente 'happy' nos segmentos 1-3"
  ],
  "average_confidence": 0.85,
  "emotion_timeline": [
    {
      "time": 0.0,
      "emotion": "happy",
      "confidence": 0.85
    }
  ]
}
```

## ğŸ¤– Modelo de FusÃ£o

O modelo de fusÃ£o Ã© uma rede neural avanÃ§ada com:
- Processamento individual para cada modalidade
- Camadas de normalizaÃ§Ã£o e dropout
- FusÃ£o adaptativa com pesos aprendidos
- Suporte para caracterÃ­sticas adicionais
- Treinamento contÃ­nuo com pseudo-labels

## ğŸ”„ Treinamento ContÃ­nuo

O modelo aprende continuamente atravÃ©s de:
1. Pseudo-labels gerados pela mÃ©dia ponderada das modalidades
2. Pesos adaptativos para cada modalidade
3. ValidaÃ§Ã£o com early stopping
4. Checkpointing automÃ¡tico
5. Monitoramento de mÃ©tricas

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request 